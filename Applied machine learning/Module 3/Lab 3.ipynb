{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "1. Load the two datasets.\n",
    "2. Select a classifier, train a model on dataset A. \n",
    "3. Evaluate the generalization error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets\n",
    "X_data_A = pd.read_csv(r\"features_dataset_A.csv\", header=None, delimiter=';').to_numpy()\n",
    "y_data_A = pd.read_csv(r\"targets_dataset_A.csv\", header=None).to_numpy()\n",
    "\n",
    "X_data_B = pd.read_csv(r\"features_dataset_B.csv\", header=None, delimiter=';').to_numpy()\n",
    "y_data_B = pd.read_csv(r\"targets_dataset_B.csv\", header=None).to_numpy()\n",
    "\n",
    "# 80/20 train test split\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(X_data_A, y_data_A, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Evaluation metrics for Random forest classifier ***\n",
      "\n",
      "    ROC/AUC = 0.8546\n",
      "    Mean cross validation with ROC/AUC scores and k=100 folds = 0.8697\n",
      "\n",
      "Based on this very basic evaluation, we can see that the model has a good fit since the ROC/AUC scoreas are close to 1 and do not differ much.\n"
     ]
    }
   ],
   "source": [
    "# Training a standard random forest classsifier\n",
    "rand_forest_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# K fold cross validation with\n",
    "k=100\n",
    "rand_forest_mean_cross_val = np.mean(cross_val_score(rand_forest_model, X_train_A, y_train_A, cv=k, scoring='roc_auc'))\n",
    "\n",
    "# Fitting the model and making predictions\n",
    "rand_forest_model_A = rand_forest_model.fit(X_train_A, y_train_A)\n",
    "predictions_A = rand_forest_model_A.predict(X_test_A)\n",
    "\n",
    "# Getting the R2 score on the predictions\n",
    "rand_forest_rocauc = roc_auc_score(y_true=y_test_A, y_score=predictions_A)\n",
    "\n",
    "# Printing the values\n",
    "print(f\"*** Evaluation metrics for Random forest classifier ***\\n\")\n",
    "print(f\"    ROC/AUC = {rand_forest_rocauc:.4f}\")\n",
    "print(f\"    Mean cross validation with ROC/AUC scores and k={k} folds = {rand_forest_mean_cross_val:.4f}\\n\")\n",
    "print(f\"Based on this very basic evaluation, we can see that the model has a good fit since the ROC/AUC scoreas are close to 1 and do not differ much.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "1. Test the model on the dataset B (production dataset).\n",
    "2. How well is the trained classifier performing on the production dataset? Is it better or worse than on the generalization error \n",
    "of dataset A?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3\n",
    "1. Measure how the features are changing over time (concept drift) with the Kolmogorov-Smirnov Test to see if the distribution of the features remains constant or if they are changing over time. \n",
    "    - Your reference distribution can be the 250 first values of dataset A, called FEATURE_SAMPLES_DS_A. \n",
    "    - First try to do a K-S Test between FEATURE_SAMPLES_DS_A and for indeces 0 to 250 of the production dataset B. \n",
    "    - Do they having the same distribution? Can we reject the null hypothesis?\n",
    "2. Do a K-S Test between FEATURE_SAMPLES_DS_A and for indeces 250 to 500 of the production dataset B. \n",
    "    - Are they having the same distribution? \n",
    "    - Can we reject the null hypothesis?\n",
    "3. Try other windows in the production dataset B. \n",
    "    - Estimate where the concept drift starts (at which index in dataset B)? \n",
    "    - Which type of concept shift occurs: abrupt gradual or incremental?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4\n",
    "1. Retrain the model with parts of dataset B occurring after the start of the concept drift (which could be estimated in the third task). \n",
    "    - How is the generalization error now? Is it improved?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
